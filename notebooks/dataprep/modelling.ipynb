{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import packages and extracted python class for classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################################\n",
    "#                                                            #\n",
    "#    Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "#    Machine Learning for the Quantified Self                #\n",
    "#    Springer                                                #\n",
    "#    Chapter 7                                               #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "# Updated by Dave Ebbelaar on 12-01-2023\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class ClassificationAlgorithms:\n",
    "\n",
    "    # Forward selection for classification which selects a pre-defined number of features (max_features)\n",
    "    # that show the best accuracy. We assume a decision tree learning for this purpose, but\n",
    "    # this can easily be changed. It return the best features.\n",
    "    def forward_selection(self, max_features, X_train, y_train):\n",
    "        # Start with no features.\n",
    "        ordered_features = []\n",
    "        ordered_scores = []\n",
    "        selected_features = []\n",
    "        ca = ClassificationAlgorithms()\n",
    "        prev_best_perf = 0\n",
    "\n",
    "        # Select the appropriate number of features.\n",
    "        for i in range(0, max_features):\n",
    "            print(i)\n",
    "\n",
    "            # Determine the features left to select.\n",
    "            features_left = list(set(X_train.columns) - set(selected_features))\n",
    "            best_perf = 0\n",
    "            best_attribute = \"\"\n",
    "\n",
    "            # For all features we can still select...\n",
    "            for f in features_left:\n",
    "                temp_selected_features = copy.deepcopy(selected_features)\n",
    "                temp_selected_features.append(f)\n",
    "\n",
    "                # Determine the accuracy of a decision tree learner if we were to add\n",
    "                # the feature.\n",
    "                (\n",
    "                    pred_y_train,\n",
    "                    pred_y_test,\n",
    "                    prob_training_y,\n",
    "                    prob_test_y,\n",
    "                ) = ca.decision_tree(\n",
    "                    X_train[temp_selected_features],\n",
    "                    y_train,\n",
    "                    X_train[temp_selected_features],\n",
    "                )\n",
    "                perf = accuracy_score(y_train, pred_y_train)\n",
    "\n",
    "                # If the performance is better than what we have seen so far (we aim for high accuracy)\n",
    "                # we set the current feature to the best feature and the same for the best performance.\n",
    "                if perf > best_perf:\n",
    "                    best_perf = perf\n",
    "                    best_feature = f\n",
    "            # We select the feature with the best performance.\n",
    "            selected_features.append(best_feature)\n",
    "            prev_best_perf = best_perf\n",
    "            ordered_features.append(best_feature)\n",
    "            ordered_scores.append(best_perf)\n",
    "        return selected_features, ordered_features, ordered_scores\n",
    "\n",
    "    # Apply a neural network for classification upon the training data (with the specified composition of\n",
    "    # hidden layers and number of iterations), and use the created network to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def feedforward_neural_network(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        hidden_layer_sizes=(100,),\n",
    "        max_iter=2000,\n",
    "        activation=\"logistic\",\n",
    "        alpha=0.0001,\n",
    "        learning_rate=\"adaptive\",\n",
    "        gridsearch=True,\n",
    "        print_model_details=False,\n",
    "    ):\n",
    "\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [\n",
    "                {\n",
    "                    \"hidden_layer_sizes\": [\n",
    "                        (5,),\n",
    "                        (10,),\n",
    "                        (25,),\n",
    "                        (100,),\n",
    "                        (\n",
    "                            100,\n",
    "                            5,\n",
    "                        ),\n",
    "                        (\n",
    "                            100,\n",
    "                            10,\n",
    "                        ),\n",
    "                    ],\n",
    "                    \"activation\": [activation],\n",
    "                    \"learning_rate\": [learning_rate],\n",
    "                    \"max_iter\": [1000, 2000],\n",
    "                    \"alpha\": [alpha],\n",
    "                }\n",
    "            ]\n",
    "            nn = GridSearchCV(\n",
    "                MLPClassifier(), tuned_parameters, cv=5, scoring=\"accuracy\"\n",
    "            )\n",
    "        else:\n",
    "            # Create the model\n",
    "            nn = MLPClassifier(\n",
    "                hidden_layer_sizes=hidden_layer_sizes,\n",
    "                activation=activation,\n",
    "                max_iter=max_iter,\n",
    "                learning_rate=learning_rate,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "\n",
    "        # Fit the model\n",
    "        nn.fit(\n",
    "            train_X,\n",
    "            train_y.values.ravel(),\n",
    "        )\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(nn.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            nn = nn.best_estimator_\n",
    "\n",
    "        # Apply the model\n",
    "        pred_prob_training_y = nn.predict_proba(train_X)\n",
    "        pred_prob_test_y = nn.predict_proba(test_X)\n",
    "        pred_training_y = nn.predict(train_X)\n",
    "        pred_test_y = nn.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=nn.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=nn.classes_)\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a support vector machine for classification upon the training data (with the specified value for\n",
    "    # C, epsilon and the kernel function), and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def support_vector_machine_with_kernel(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        kernel=\"rbf\",\n",
    "        C=1,\n",
    "        gamma=1e-3,\n",
    "        gridsearch=True,\n",
    "        print_model_details=False,\n",
    "    ):\n",
    "        # Create the model\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [\n",
    "                {\"kernel\": [\"rbf\", \"poly\"], \"gamma\": [1e-3, 1e-4], \"C\": [1, 10, 100]}\n",
    "            ]\n",
    "            svm = GridSearchCV(\n",
    "                SVC(probability=True), tuned_parameters, cv=5, scoring=\"accuracy\"\n",
    "            )\n",
    "        else:\n",
    "            svm = SVC(\n",
    "                C=C, kernel=kernel, gamma=gamma, probability=True, cache_size=7000\n",
    "            )\n",
    "\n",
    "        # Fit the model\n",
    "        svm.fit(train_X, train_y.values.ravel())\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(svm.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            svm = svm.best_estimator_\n",
    "\n",
    "        # Apply the model\n",
    "        pred_prob_training_y = svm.predict_proba(train_X)\n",
    "        pred_prob_test_y = svm.predict_proba(test_X)\n",
    "        pred_training_y = svm.predict(train_X)\n",
    "        pred_test_y = svm.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=svm.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=svm.classes_)\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a support vector machine for classification upon the training data (with the specified value for\n",
    "    # C, epsilon and the kernel function), and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def support_vector_machine_without_kernel(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        C=1,\n",
    "        tol=1e-3,\n",
    "        max_iter=1000,\n",
    "        gridsearch=True,\n",
    "        print_model_details=False,\n",
    "    ):\n",
    "        # Create the model\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [\n",
    "                {\"max_iter\": [1000, 2000], \"tol\": [1e-3, 1e-4], \"C\": [1, 10, 100]}\n",
    "            ]\n",
    "            svm = GridSearchCV(LinearSVC(), tuned_parameters, cv=5, scoring=\"accuracy\")\n",
    "        else:\n",
    "            svm = LinearSVC(C=C, tol=tol, max_iter=max_iter)\n",
    "\n",
    "        # Fit the model\n",
    "        svm.fit(train_X, train_y.values.ravel())\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(svm.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            svm = svm.best_estimator_\n",
    "\n",
    "        # Apply the model\n",
    "\n",
    "        distance_training_platt = 1 / (1 + np.exp(svm.decision_function(train_X)))\n",
    "        pred_prob_training_y = (\n",
    "            distance_training_platt / distance_training_platt.sum(axis=1)[:, None]\n",
    "        )\n",
    "        distance_test_platt = 1 / (1 + np.exp(svm.decision_function(test_X)))\n",
    "        pred_prob_test_y = (\n",
    "            distance_test_platt / distance_test_platt.sum(axis=1)[:, None]\n",
    "        )\n",
    "        pred_training_y = svm.predict(train_X)\n",
    "        pred_test_y = svm.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=svm.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=svm.classes_)\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a nearest neighbor approach for classification upon the training data (with the specified value for\n",
    "    # k), and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def k_nearest_neighbor(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        n_neighbors=5,\n",
    "        gridsearch=True,\n",
    "        print_model_details=False,\n",
    "    ):\n",
    "        # Create the model\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [{\"n_neighbors\": [1, 2, 5, 10]}]\n",
    "            knn = GridSearchCV(\n",
    "                KNeighborsClassifier(), tuned_parameters, cv=5, scoring=\"accuracy\"\n",
    "            )\n",
    "        else:\n",
    "            knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "        # Fit the model\n",
    "        knn.fit(train_X, train_y)\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(knn.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            knn = knn.best_estimator_\n",
    "\n",
    "        # Apply the model\n",
    "        pred_prob_training_y = knn.predict_proba(train_X)\n",
    "        pred_prob_test_y = knn.predict_proba(test_X)\n",
    "        pred_training_y = knn.predict(train_X)\n",
    "        pred_test_y = knn.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=knn.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=knn.classes_)\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a decision tree approach for classification upon the training data (with the specified value for\n",
    "    # the minimum samples in the leaf, and the export path and files if print_model_details=True)\n",
    "    # and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def decision_tree(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        min_samples_leaf=50,\n",
    "        criterion=\"gini\",\n",
    "        print_model_details=False,\n",
    "        export_tree_path=\"Example_graphs/Chapter7/\",\n",
    "        export_tree_name=\"tree.dot\",\n",
    "        gridsearch=True,\n",
    "    ):\n",
    "        # Create the model\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [\n",
    "                {\n",
    "                    \"min_samples_leaf\": [2, 10, 50, 100, 200],\n",
    "                    \"criterion\": [\"gini\", \"entropy\"],\n",
    "                }\n",
    "            ]\n",
    "            dtree = GridSearchCV(\n",
    "                DecisionTreeClassifier(), tuned_parameters, cv=5, scoring=\"accuracy\"\n",
    "            )\n",
    "        else:\n",
    "            dtree = DecisionTreeClassifier(\n",
    "                min_samples_leaf=min_samples_leaf, criterion=criterion\n",
    "            )\n",
    "\n",
    "        # Fit the model\n",
    "\n",
    "        dtree.fit(train_X, train_y)\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(dtree.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            dtree = dtree.best_estimator_\n",
    "\n",
    "        # Apply the model\n",
    "        pred_prob_training_y = dtree.predict_proba(train_X)\n",
    "        pred_prob_test_y = dtree.predict_proba(test_X)\n",
    "        pred_training_y = dtree.predict(train_X)\n",
    "        pred_test_y = dtree.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(\n",
    "            pred_prob_training_y, columns=dtree.classes_\n",
    "        )\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=dtree.classes_)\n",
    "\n",
    "        if print_model_details:\n",
    "            ordered_indices = [\n",
    "                i[0]\n",
    "                for i in sorted(\n",
    "                    enumerate(dtree.feature_importances_),\n",
    "                    key=lambda x: x[1],\n",
    "                    reverse=True,\n",
    "                )\n",
    "            ]\n",
    "            print(\"Feature importance decision tree:\")\n",
    "            for i in range(0, len(dtree.feature_importances_)):\n",
    "                print(\n",
    "                    train_X.columns[ordered_indices[i]],\n",
    "                )\n",
    "                print(\n",
    "                    \" & \",\n",
    "                )\n",
    "                print(dtree.feature_importances_[ordered_indices[i]])\n",
    "            tree.export_graphviz(\n",
    "                dtree,\n",
    "                out_file=export_tree_path + export_tree_name,\n",
    "                feature_names=train_X.columns,\n",
    "                class_names=dtree.classes_,\n",
    "            )\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a naive bayes approach for classification upon the training data\n",
    "    # and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def naive_bayes(self, train_X, train_y, test_X):\n",
    "        # Create the model\n",
    "        nb = GaussianNB()\n",
    "\n",
    "        # Fit the model\n",
    "        nb.fit(train_X, train_y)\n",
    "\n",
    "        # Apply the model\n",
    "        pred_prob_training_y = nb.predict_proba(train_X)\n",
    "        pred_prob_test_y = nb.predict_proba(test_X)\n",
    "        pred_training_y = nb.predict(train_X)\n",
    "        pred_test_y = nb.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=nb.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=nb.classes_)\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a random forest approach for classification upon the training data (with the specified value for\n",
    "    # the minimum samples in the leaf, the number of trees, and if we should print some of the details of the\n",
    "    # model print_model_details=True) and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def random_forest(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        n_estimators=10,\n",
    "        min_samples_leaf=5,\n",
    "        criterion=\"gini\",\n",
    "        print_model_details=False,\n",
    "        gridsearch=True,\n",
    "    ):\n",
    "\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [\n",
    "                {\n",
    "                    \"min_samples_leaf\": [2, 10, 50, 100, 200],\n",
    "                    \"n_estimators\": [10, 50, 100],\n",
    "                    \"criterion\": [\"gini\", \"entropy\"],\n",
    "                }\n",
    "            ]\n",
    "            rf = GridSearchCV(\n",
    "                RandomForestClassifier(), tuned_parameters, cv=5, scoring=\"accuracy\"\n",
    "            )\n",
    "        else:\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                criterion=criterion,\n",
    "            )\n",
    "\n",
    "        # Fit the model\n",
    "\n",
    "        rf.fit(train_X, train_y)\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(rf.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            rf = rf.best_estimator_\n",
    "\n",
    "        pred_prob_training_y = rf.predict_proba(train_X)\n",
    "        pred_prob_test_y = rf.predict_proba(test_X)\n",
    "        pred_training_y = rf.predict(train_X)\n",
    "        pred_test_y = rf.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=rf.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=rf.classes_)\n",
    "\n",
    "        if print_model_details:\n",
    "            ordered_indices = [\n",
    "                i[0]\n",
    "                for i in sorted(\n",
    "                    enumerate(rf.feature_importances_), key=lambda x: x[1], reverse=True\n",
    "                )\n",
    "            ]\n",
    "            print(\"Feature importance random forest:\")\n",
    "            for i in range(0, len(rf.feature_importances_)):\n",
    "                print(\n",
    "                    train_X.columns[ordered_indices[i]],\n",
    "                )\n",
    "                print(\n",
    "                    \" & \",\n",
    "                )\n",
    "                print(rf.feature_importances_[ordered_indices[i]])\n",
    "\n",
    "        return (\n",
    "            pred_training_y,\n",
    "            pred_test_y,\n",
    "            frame_prob_training_y,\n",
    "            frame_prob_test_y,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../data/interim/df_freq_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>label</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>trial</th>\n",
       "      <th>age</th>\n",
       "      <th>...</th>\n",
       "      <th>acc_z_entropy</th>\n",
       "      <th>gyro_x_fft</th>\n",
       "      <th>gyro_x_psd</th>\n",
       "      <th>gyro_x_entropy</th>\n",
       "      <th>gyro_y_fft</th>\n",
       "      <th>gyro_y_psd</th>\n",
       "      <th>gyro_y_entropy</th>\n",
       "      <th>gyro_z_fft</th>\n",
       "      <th>gyro_z_psd</th>\n",
       "      <th>gyro_z_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.068994</td>\n",
       "      <td>-6.987095</td>\n",
       "      <td>5.288118</td>\n",
       "      <td>1.340131</td>\n",
       "      <td>0.254273</td>\n",
       "      <td>0.553948</td>\n",
       "      <td>BSC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552713</td>\n",
       "      <td>2.237175</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.920616</td>\n",
       "      <td>0.363307</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>1.282563</td>\n",
       "      <td>0.614725</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>1.354238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.470804</td>\n",
       "      <td>-1.801783</td>\n",
       "      <td>8.929937</td>\n",
       "      <td>1.701367</td>\n",
       "      <td>0.193919</td>\n",
       "      <td>0.328752</td>\n",
       "      <td>BSC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697146</td>\n",
       "      <td>1.886452</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.905243</td>\n",
       "      <td>0.727568</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>1.175494</td>\n",
       "      <td>1.138423</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>1.415926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.901018</td>\n",
       "      <td>13.441229</td>\n",
       "      <td>7.822155</td>\n",
       "      <td>-0.351828</td>\n",
       "      <td>0.343581</td>\n",
       "      <td>-0.851439</td>\n",
       "      <td>BSC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.074713</td>\n",
       "      <td>3.329770</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>1.203505</td>\n",
       "      <td>1.706857</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.936669</td>\n",
       "      <td>3.093796</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.577150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-5.150726</td>\n",
       "      <td>6.613287</td>\n",
       "      <td>3.195224</td>\n",
       "      <td>-0.107008</td>\n",
       "      <td>-1.118601</td>\n",
       "      <td>-0.638797</td>\n",
       "      <td>BSC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.461785</td>\n",
       "      <td>9.083104</td>\n",
       "      <td>0.043227</td>\n",
       "      <td>1.271543</td>\n",
       "      <td>2.602335</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>1.358140</td>\n",
       "      <td>3.267480</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>0.745639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-4.931030</td>\n",
       "      <td>5.574285</td>\n",
       "      <td>6.261700</td>\n",
       "      <td>-0.019441</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>0.115133</td>\n",
       "      <td>BSC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.176317</td>\n",
       "      <td>1.104385</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>1.366146</td>\n",
       "      <td>3.538939</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>1.667364</td>\n",
       "      <td>0.830726</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>1.440714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169385</th>\n",
       "      <td>0.586223</td>\n",
       "      <td>5.443400</td>\n",
       "      <td>1.029336</td>\n",
       "      <td>-0.737727</td>\n",
       "      <td>1.443459</td>\n",
       "      <td>1.335214</td>\n",
       "      <td>WAL</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.343248</td>\n",
       "      <td>7.967362</td>\n",
       "      <td>0.019501</td>\n",
       "      <td>0.917517</td>\n",
       "      <td>3.978832</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>1.133685</td>\n",
       "      <td>9.439387</td>\n",
       "      <td>0.030438</td>\n",
       "      <td>0.913799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169386</th>\n",
       "      <td>2.286937</td>\n",
       "      <td>11.130139</td>\n",
       "      <td>-1.195925</td>\n",
       "      <td>1.260658</td>\n",
       "      <td>-0.457721</td>\n",
       "      <td>-0.981431</td>\n",
       "      <td>WAL</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.502760</td>\n",
       "      <td>7.253110</td>\n",
       "      <td>0.017147</td>\n",
       "      <td>0.899968</td>\n",
       "      <td>6.142548</td>\n",
       "      <td>0.022069</td>\n",
       "      <td>1.258938</td>\n",
       "      <td>8.046604</td>\n",
       "      <td>0.023517</td>\n",
       "      <td>0.994088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169387</th>\n",
       "      <td>-5.076769</td>\n",
       "      <td>14.152212</td>\n",
       "      <td>2.039380</td>\n",
       "      <td>-2.541764</td>\n",
       "      <td>-1.378478</td>\n",
       "      <td>2.412521</td>\n",
       "      <td>WAL</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.377740</td>\n",
       "      <td>5.479225</td>\n",
       "      <td>0.007237</td>\n",
       "      <td>1.008082</td>\n",
       "      <td>3.459003</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>1.726653</td>\n",
       "      <td>5.018624</td>\n",
       "      <td>0.007999</td>\n",
       "      <td>1.087638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169388</th>\n",
       "      <td>1.671382</td>\n",
       "      <td>11.554947</td>\n",
       "      <td>-1.548809</td>\n",
       "      <td>0.596082</td>\n",
       "      <td>-1.174618</td>\n",
       "      <td>-0.679053</td>\n",
       "      <td>WAL</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.449325</td>\n",
       "      <td>7.200013</td>\n",
       "      <td>0.037785</td>\n",
       "      <td>1.189187</td>\n",
       "      <td>3.266761</td>\n",
       "      <td>0.013842</td>\n",
       "      <td>1.523287</td>\n",
       "      <td>7.761068</td>\n",
       "      <td>0.044975</td>\n",
       "      <td>1.151021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169389</th>\n",
       "      <td>2.128992</td>\n",
       "      <td>14.084110</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>-1.022787</td>\n",
       "      <td>0.162796</td>\n",
       "      <td>0.609964</td>\n",
       "      <td>WAL</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.489078</td>\n",
       "      <td>5.413554</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>1.100799</td>\n",
       "      <td>3.660993</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>1.503933</td>\n",
       "      <td>5.857457</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>1.177872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167662 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           acc_x      acc_y     acc_z    gyro_x    gyro_y    gyro_z label  \\\n",
       "2      -0.068994  -6.987095  5.288118  1.340131  0.254273  0.553948   BSC   \n",
       "3      -0.470804  -1.801783  8.929937  1.701367  0.193919  0.328752   BSC   \n",
       "4      -9.901018  13.441229  7.822155 -0.351828  0.343581 -0.851439   BSC   \n",
       "5      -5.150726   6.613287  3.195224 -0.107008 -1.118601 -0.638797   BSC   \n",
       "6      -4.931030   5.574285  6.261700 -0.019441  0.012431  0.115133   BSC   \n",
       "...          ...        ...       ...       ...       ...       ...   ...   \n",
       "169385  0.586223   5.443400  1.029336 -0.737727  1.443459  1.335214   WAL   \n",
       "169386  2.286937  11.130139 -1.195925  1.260658 -0.457721 -0.981431   WAL   \n",
       "169387 -5.076769  14.152212  2.039380 -2.541764 -1.378478  2.412521   WAL   \n",
       "169388  1.671382  11.554947 -1.548809  0.596082 -1.174618 -0.679053   WAL   \n",
       "169389  2.128992  14.084110  0.907692 -1.022787  0.162796  0.609964   WAL   \n",
       "\n",
       "        subject_id  trial   age  ...  acc_z_entropy  gyro_x_fft gyro_x_psd  \\\n",
       "2              1.0    1.0  32.0  ...       0.552713    2.237175   0.000722   \n",
       "3              1.0    1.0  32.0  ...       0.697146    1.886452   0.000581   \n",
       "4              1.0    1.0  32.0  ...       1.074713    3.329770   0.005098   \n",
       "5              1.0    1.0  32.0  ...       1.461785    9.083104   0.043227   \n",
       "6              1.0    1.0  32.0  ...       1.176317    1.104385   0.000710   \n",
       "...            ...    ...   ...  ...            ...         ...        ...   \n",
       "169385        67.0    1.0  23.0  ...       1.343248    7.967362   0.019501   \n",
       "169386        67.0    1.0  23.0  ...       1.502760    7.253110   0.017147   \n",
       "169387        67.0    1.0  23.0  ...       1.377740    5.479225   0.007237   \n",
       "169388        67.0    1.0  23.0  ...       1.449325    7.200013   0.037785   \n",
       "169389        67.0    1.0  23.0  ...       1.489078    5.413554   0.007328   \n",
       "\n",
       "        gyro_x_entropy  gyro_y_fft  gyro_y_psd  gyro_y_entropy  gyro_z_fft  \\\n",
       "2             0.920616    0.363307    0.000888        1.282563    0.614725   \n",
       "3             0.905243    0.727568    0.000668        1.175494    1.138423   \n",
       "4             1.203505    1.706857    0.000565        0.936669    3.093796   \n",
       "5             1.271543    2.602335    0.005232        1.358140    3.267480   \n",
       "6             1.366146    3.538939    0.009700        1.667364    0.830726   \n",
       "...                ...         ...         ...             ...         ...   \n",
       "169385        0.917517    3.978832    0.003662        1.133685    9.439387   \n",
       "169386        0.899968    6.142548    0.022069        1.258938    8.046604   \n",
       "169387        1.008082    3.459003    0.006143        1.726653    5.018624   \n",
       "169388        1.189187    3.266761    0.013842        1.523287    7.761068   \n",
       "169389        1.100799    3.660993    0.006653        1.503933    5.857457   \n",
       "\n",
       "        gyro_z_psd  gyro_z_entropy  \n",
       "2         0.000267        1.354238  \n",
       "3         0.000603        1.415926  \n",
       "4         0.002285        0.577150  \n",
       "5         0.006812        0.745639  \n",
       "6         0.003255        1.440714  \n",
       "...            ...             ...  \n",
       "169385    0.030438        0.913799  \n",
       "169386    0.023517        0.994088  \n",
       "169387    0.007999        1.087638  \n",
       "169388    0.044975        1.151021  \n",
       "169389    0.009425        1.177872  \n",
       "\n",
       "[167662 rows x 33 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.drop([\"subject_id\", \"trial\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(\"label\", axis =1)\n",
    "y = df_train[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>...</th>\n",
       "      <th>acc_z_entropy</th>\n",
       "      <th>gyro_x_fft</th>\n",
       "      <th>gyro_x_psd</th>\n",
       "      <th>gyro_x_entropy</th>\n",
       "      <th>gyro_y_fft</th>\n",
       "      <th>gyro_y_psd</th>\n",
       "      <th>gyro_y_entropy</th>\n",
       "      <th>gyro_z_fft</th>\n",
       "      <th>gyro_z_psd</th>\n",
       "      <th>gyro_z_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.068994</td>\n",
       "      <td>-6.987095</td>\n",
       "      <td>5.288118</td>\n",
       "      <td>1.340131</td>\n",
       "      <td>0.254273</td>\n",
       "      <td>0.553948</td>\n",
       "      <td>32.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552713</td>\n",
       "      <td>2.237175</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.920616</td>\n",
       "      <td>0.363307</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>1.282563</td>\n",
       "      <td>0.614725</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>1.354238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.470804</td>\n",
       "      <td>-1.801783</td>\n",
       "      <td>8.929937</td>\n",
       "      <td>1.701367</td>\n",
       "      <td>0.193919</td>\n",
       "      <td>0.328752</td>\n",
       "      <td>32.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697146</td>\n",
       "      <td>1.886452</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.905243</td>\n",
       "      <td>0.727568</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>1.175494</td>\n",
       "      <td>1.138423</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>1.415926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.901018</td>\n",
       "      <td>13.441229</td>\n",
       "      <td>7.822155</td>\n",
       "      <td>-0.351828</td>\n",
       "      <td>0.343581</td>\n",
       "      <td>-0.851439</td>\n",
       "      <td>32.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>1.074713</td>\n",
       "      <td>3.329770</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>1.203505</td>\n",
       "      <td>1.706857</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.936669</td>\n",
       "      <td>3.093796</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.577150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-5.150726</td>\n",
       "      <td>6.613287</td>\n",
       "      <td>3.195224</td>\n",
       "      <td>-0.107008</td>\n",
       "      <td>-1.118601</td>\n",
       "      <td>-0.638797</td>\n",
       "      <td>32.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>1.461785</td>\n",
       "      <td>9.083104</td>\n",
       "      <td>0.043227</td>\n",
       "      <td>1.271543</td>\n",
       "      <td>2.602335</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>1.358140</td>\n",
       "      <td>3.267480</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>0.745639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-4.931030</td>\n",
       "      <td>5.574285</td>\n",
       "      <td>6.261700</td>\n",
       "      <td>-0.019441</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>0.115133</td>\n",
       "      <td>32.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>1.176317</td>\n",
       "      <td>1.104385</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>1.366146</td>\n",
       "      <td>3.538939</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>1.667364</td>\n",
       "      <td>0.830726</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>1.440714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169385</th>\n",
       "      <td>0.586223</td>\n",
       "      <td>5.443400</td>\n",
       "      <td>1.029336</td>\n",
       "      <td>-0.737727</td>\n",
       "      <td>1.443459</td>\n",
       "      <td>1.335214</td>\n",
       "      <td>23.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>1.343248</td>\n",
       "      <td>7.967362</td>\n",
       "      <td>0.019501</td>\n",
       "      <td>0.917517</td>\n",
       "      <td>3.978832</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>1.133685</td>\n",
       "      <td>9.439387</td>\n",
       "      <td>0.030438</td>\n",
       "      <td>0.913799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169386</th>\n",
       "      <td>2.286937</td>\n",
       "      <td>11.130139</td>\n",
       "      <td>-1.195925</td>\n",
       "      <td>1.260658</td>\n",
       "      <td>-0.457721</td>\n",
       "      <td>-0.981431</td>\n",
       "      <td>23.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>1.502760</td>\n",
       "      <td>7.253110</td>\n",
       "      <td>0.017147</td>\n",
       "      <td>0.899968</td>\n",
       "      <td>6.142548</td>\n",
       "      <td>0.022069</td>\n",
       "      <td>1.258938</td>\n",
       "      <td>8.046604</td>\n",
       "      <td>0.023517</td>\n",
       "      <td>0.994088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169387</th>\n",
       "      <td>-5.076769</td>\n",
       "      <td>14.152212</td>\n",
       "      <td>2.039380</td>\n",
       "      <td>-2.541764</td>\n",
       "      <td>-1.378478</td>\n",
       "      <td>2.412521</td>\n",
       "      <td>23.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>1.377740</td>\n",
       "      <td>5.479225</td>\n",
       "      <td>0.007237</td>\n",
       "      <td>1.008082</td>\n",
       "      <td>3.459003</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>1.726653</td>\n",
       "      <td>5.018624</td>\n",
       "      <td>0.007999</td>\n",
       "      <td>1.087638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169388</th>\n",
       "      <td>1.671382</td>\n",
       "      <td>11.554947</td>\n",
       "      <td>-1.548809</td>\n",
       "      <td>0.596082</td>\n",
       "      <td>-1.174618</td>\n",
       "      <td>-0.679053</td>\n",
       "      <td>23.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>1.449325</td>\n",
       "      <td>7.200013</td>\n",
       "      <td>0.037785</td>\n",
       "      <td>1.189187</td>\n",
       "      <td>3.266761</td>\n",
       "      <td>0.013842</td>\n",
       "      <td>1.523287</td>\n",
       "      <td>7.761068</td>\n",
       "      <td>0.044975</td>\n",
       "      <td>1.151021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169389</th>\n",
       "      <td>2.128992</td>\n",
       "      <td>14.084110</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>-1.022787</td>\n",
       "      <td>0.162796</td>\n",
       "      <td>0.609964</td>\n",
       "      <td>23.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>1.489078</td>\n",
       "      <td>5.413554</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>1.100799</td>\n",
       "      <td>3.660993</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>1.503933</td>\n",
       "      <td>5.857457</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>1.177872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167662 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           acc_x      acc_y     acc_z    gyro_x    gyro_y    gyro_z   age  \\\n",
       "2      -0.068994  -6.987095  5.288118  1.340131  0.254273  0.553948  32.0   \n",
       "3      -0.470804  -1.801783  8.929937  1.701367  0.193919  0.328752  32.0   \n",
       "4      -9.901018  13.441229  7.822155 -0.351828  0.343581 -0.851439  32.0   \n",
       "5      -5.150726   6.613287  3.195224 -0.107008 -1.118601 -0.638797  32.0   \n",
       "6      -4.931030   5.574285  6.261700 -0.019441  0.012431  0.115133  32.0   \n",
       "...          ...        ...       ...       ...       ...       ...   ...   \n",
       "169385  0.586223   5.443400  1.029336 -0.737727  1.443459  1.335214  23.0   \n",
       "169386  2.286937  11.130139 -1.195925  1.260658 -0.457721 -0.981431  23.0   \n",
       "169387 -5.076769  14.152212  2.039380 -2.541764 -1.378478  2.412521  23.0   \n",
       "169388  1.671382  11.554947 -1.548809  0.596082 -1.174618 -0.679053  23.0   \n",
       "169389  2.128992  14.084110  0.907692 -1.022787  0.162796  0.609964  23.0   \n",
       "\n",
       "        height  weight gender  ...  acc_z_entropy  gyro_x_fft  gyro_x_psd  \\\n",
       "2        180.0    85.0      M  ...       0.552713    2.237175    0.000722   \n",
       "3        180.0    85.0      M  ...       0.697146    1.886452    0.000581   \n",
       "4        180.0    85.0      M  ...       1.074713    3.329770    0.005098   \n",
       "5        180.0    85.0      M  ...       1.461785    9.083104    0.043227   \n",
       "6        180.0    85.0      M  ...       1.176317    1.104385    0.000710   \n",
       "...        ...     ...    ...  ...            ...         ...         ...   \n",
       "169385   180.0    67.0      M  ...       1.343248    7.967362    0.019501   \n",
       "169386   180.0    67.0      M  ...       1.502760    7.253110    0.017147   \n",
       "169387   180.0    67.0      M  ...       1.377740    5.479225    0.007237   \n",
       "169388   180.0    67.0      M  ...       1.449325    7.200013    0.037785   \n",
       "169389   180.0    67.0      M  ...       1.489078    5.413554    0.007328   \n",
       "\n",
       "        gyro_x_entropy  gyro_y_fft  gyro_y_psd  gyro_y_entropy  gyro_z_fft  \\\n",
       "2             0.920616    0.363307    0.000888        1.282563    0.614725   \n",
       "3             0.905243    0.727568    0.000668        1.175494    1.138423   \n",
       "4             1.203505    1.706857    0.000565        0.936669    3.093796   \n",
       "5             1.271543    2.602335    0.005232        1.358140    3.267480   \n",
       "6             1.366146    3.538939    0.009700        1.667364    0.830726   \n",
       "...                ...         ...         ...             ...         ...   \n",
       "169385        0.917517    3.978832    0.003662        1.133685    9.439387   \n",
       "169386        0.899968    6.142548    0.022069        1.258938    8.046604   \n",
       "169387        1.008082    3.459003    0.006143        1.726653    5.018624   \n",
       "169388        1.189187    3.266761    0.013842        1.523287    7.761068   \n",
       "169389        1.100799    3.660993    0.006653        1.503933    5.857457   \n",
       "\n",
       "        gyro_z_psd  gyro_z_entropy  \n",
       "2         0.000267        1.354238  \n",
       "3         0.000603        1.415926  \n",
       "4         0.002285        0.577150  \n",
       "5         0.006812        0.745639  \n",
       "6         0.003255        1.440714  \n",
       "...            ...             ...  \n",
       "169385    0.030438        0.913799  \n",
       "169386    0.023517        0.994088  \n",
       "169387    0.007999        1.087638  \n",
       "169388    0.044975        1.151021  \n",
       "169389    0.009425        1.177872  \n",
       "\n",
       "[167662 rows x 30 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         BSC\n",
       "3         BSC\n",
       "4         BSC\n",
       "5         BSC\n",
       "6         BSC\n",
       "         ... \n",
       "169385    WAL\n",
       "169386    WAL\n",
       "169387    WAL\n",
       "169388    WAL\n",
       "169389    WAL\n",
       "Name: label, Length: 167662, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with stratify\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "STD    37368\n",
       "WAL    36817\n",
       "SIT     8956\n",
       "JOG     8653\n",
       "JUM     8347\n",
       "LYI     7581\n",
       "STU     4910\n",
       "STN     4504\n",
       "CSO     2335\n",
       "CSI     2198\n",
       "SCH     1190\n",
       "BSC      762\n",
       "SDL      644\n",
       "FKL      609\n",
       "FOL      536\n",
       "CHU      336\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "STD    12456\n",
       "WAL    12273\n",
       "SIT     2985\n",
       "JOG     2884\n",
       "JUM     2783\n",
       "LYI     2527\n",
       "STU     1637\n",
       "STN     1501\n",
       "CSO      779\n",
       "CSI      733\n",
       "SCH      397\n",
       "BSC      254\n",
       "SDL      214\n",
       "FKL      203\n",
       "FOL      178\n",
       "CHU      112\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z', 'label', 'age',\n",
       "       'height', 'weight', 'gender', 'acc_mag', 'gyro_mag', 'acc_x_fft',\n",
       "       'acc_x_psd', 'acc_x_entropy', 'acc_y_fft', 'acc_y_psd', 'acc_y_entropy',\n",
       "       'acc_z_fft', 'acc_z_psd', 'acc_z_entropy', 'gyro_x_fft', 'gyro_x_psd',\n",
       "       'gyro_x_entropy', 'gyro_y_fft', 'gyro_y_psd', 'gyro_y_entropy',\n",
       "       'gyro_z_fft', 'gyro_z_psd', 'gyro_z_entropy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features\n",
    "\n",
    "basic_features = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "subject_features = ['age', 'height', 'weight']\n",
    "time_features = [f for f in df_train.columns if any(suffix in f for suffix in ['psd', 'entropy', 'fft'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_1 = list(set(basic_features))\n",
    "feature_set_2 = list(set(basic_features + subject_features))\n",
    "feature_set_3 = list(set(feature_set_2 + time_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_feature_sets = [\n",
    "    feature_set_1,\n",
    "    feature_set_2,\n",
    "    feature_set_3 \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"Feature Set 1\",\n",
    "    \"Feature Set 2\",\n",
    "    \"Feature Set 3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the class defined abovce as learner\n",
    "\n",
    "learner = ClassificationAlgorithms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature set: 0\n",
      "\tTraining random forest, 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining KNN\n",
      "\tTraining decision tree\n",
      "\tTraining naive bayes\n",
      "Feature set: 1\n",
      "\tTraining random forest, 0\n",
      "\tTraining KNN\n",
      "\tTraining decision tree\n",
      "\tTraining naive bayes\n",
      "Feature set: 2\n",
      "\tTraining random forest, 0\n",
      "\tTraining KNN\n",
      "\tTraining decision tree\n",
      "\tTraining naive bayes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iterations = 1\n",
    "score_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i, f in zip(range(len(possible_feature_sets)), feature_names):\n",
    "    print(\"Feature set:\", i)\n",
    "    selected_train_X = X_train[possible_feature_sets[i]]\n",
    "    selected_test_X = X_test[possible_feature_sets[i]]\n",
    "\n",
    "    # First run non deterministic classifiers to average their score.\n",
    "    performance_test_nn = 0\n",
    "    performance_test_rf = 0\n",
    "\n",
    "    for it in range(0, iterations):\n",
    "        \n",
    "        print(\"\\tTraining random forest,\", it)\n",
    "        (\n",
    "            class_train_y,\n",
    "            class_test_y,\n",
    "            class_train_prob_y,\n",
    "            class_test_prob_y,\n",
    "        ) = learner.random_forest(\n",
    "            selected_train_X, y_train, selected_test_X, gridsearch=False\n",
    "        )\n",
    "        performance_test_rf += accuracy_score(y_test, class_test_y)\n",
    "\n",
    "    performance_test_nn = performance_test_nn / iterations\n",
    "    performance_test_rf = performance_test_rf / iterations\n",
    "\n",
    "    # And we run our deterministic classifiers:\n",
    "    print(\"\\tTraining KNN\")\n",
    "    (\n",
    "        class_train_y,\n",
    "        class_test_y,\n",
    "        class_train_prob_y,\n",
    "        class_test_prob_y,\n",
    "    ) = learner.k_nearest_neighbor(\n",
    "        selected_train_X, y_train, selected_test_X, gridsearch=False\n",
    "    )\n",
    "    performance_test_knn = accuracy_score(y_test, class_test_y)\n",
    "\n",
    "    print(\"\\tTraining decision tree\")\n",
    "    (\n",
    "        class_train_y,\n",
    "        class_test_y,\n",
    "        class_train_prob_y,\n",
    "        class_test_prob_y,\n",
    "    ) = learner.decision_tree(\n",
    "        selected_train_X, y_train, selected_test_X, gridsearch=False\n",
    "    )\n",
    "    performance_test_dt = accuracy_score(y_test, class_test_y)\n",
    "\n",
    "    print(\"\\tTraining naive bayes\")\n",
    "    (\n",
    "        class_train_y,\n",
    "        class_test_y,\n",
    "        class_train_prob_y,\n",
    "        class_test_prob_y,\n",
    "    ) = learner.naive_bayes(selected_train_X, y_train, selected_test_X)\n",
    "\n",
    "    performance_test_nb = accuracy_score(y_test, class_test_y)\n",
    "\n",
    "    # Save results to dataframe\n",
    "    models = [\"RF\", \"KNN\", \"DT\", \"NB\"]\n",
    "    new_scores = pd.DataFrame(\n",
    "        {\n",
    "            \"model\": models,\n",
    "            \"feature_set\": f,\n",
    "            \"accuracy\": [\n",
    "                performance_test_rf,\n",
    "                performance_test_knn,\n",
    "                performance_test_dt,\n",
    "                performance_test_nb,\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    score_df = pd.concat([score_df, new_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_df = score_df.sort_values(by=\"accuracy\", ascending=False)  # Ensure sorting is applied\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=\"model\", y=\"accuracy\", hue=\"feature_set\", data=score_df, palette=\"Set2\")  # Fixed typo\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.7, 1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gyro_y_psd',\n",
       " 'gyro_z_psd',\n",
       " 'gyro_x',\n",
       " 'weight',\n",
       " 'acc_x_fft',\n",
       " 'acc_z',\n",
       " 'gyro_x_entropy',\n",
       " 'gyro_y_fft',\n",
       " 'acc_y',\n",
       " 'gyro_x_fft',\n",
       " 'acc_x_entropy',\n",
       " 'acc_z_psd',\n",
       " 'acc_z_fft',\n",
       " 'gyro_z',\n",
       " 'gyro_z_entropy',\n",
       " 'gyro_x_psd',\n",
       " 'gyro_y',\n",
       " 'acc_y_fft',\n",
       " 'acc_x',\n",
       " 'acc_x_psd',\n",
       " 'age',\n",
       " 'height',\n",
       " 'acc_z_entropy',\n",
       " 'gyro_z_fft',\n",
       " 'acc_y_entropy',\n",
       " 'gyro_y_entropy',\n",
       " 'acc_y_psd']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
